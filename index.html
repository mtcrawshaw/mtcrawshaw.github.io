<!doctype html>
<html>
<head>
</head>
  <title>About Me</title>
  <meta name="description" content="About me">
  <meta name="keywords" content="michael crawshaw phd student george mason university computer science machine learning deep">
</html>

<body>
  <h1>Michael Crawshaw</h1>
  <p><a href="mailto:mcrawsha@gmu.edu">Email</a> | <a
  href="Resume.pdf">CV</a> | <a
  href="https://scholar.google.com/citations?user=XVrMZ_4AAAAJ&hl=en">Google Scholar</a> | <a
  href="https://github.com/mtcrawshaw">GitHub</a> | <a
  href="https://twitter.com/CrichaelMawshaw">Twitter</a> | <a
  href="https://www.linkedin.com/in/michael-crawshaw-5a6aab150/">LinkedIn</a></p>
  <img src="headshot.jpg" width="456" height="456" alt="Michael">

  <h2>About Me</h2>
  <p>I am a fourth year Ph.D. student in Computer Science at George Mason University,
  advised by Professor Mingrui Liu. My research is in the theory of optimization for
  machine learning. In particular, I am working on problems in federated learning and
  nonconvex optimization under nonsmooth settings, such as relaxed smoothness. Before
  studying at George Mason, I received a B.S. in mathematics and computer science from
  Ohio State University.</p>

  <h2>Publications</h2>
  <ul>
    <li><a href="https://openreview.net/forum?id=Yq6GKgN3RC">Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds.</a><br>
    <b>Michael Crawshaw</b>, Yajie Bao, Mingrui Liu (first two authors contributed equally)<br>
    Neural Information Processing Systems, 2023.</li>
    <li><a href="https://arxiv.org/abs/2302.07155">EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data.</a><br>
    <b>Michael Crawshaw</b>, Yajie Bao, Mingrui Liu<br>
    International Conference on Learning Representations, 2020.</li>
    <li><a href="https://arxiv.org/abs/2208.11195">Robustness to Unbounded Smoothness of Generalized SignSGD.</a><br>
    (Alphabetical order) <b>Michael Crawshaw</b>, Mingrui Liu, Francesco Orabona, Wei Zhang, Zhenxun Zhuang<br>
    Neural Information Processing Systems, 2022.</li>
    <li><a href="https://arxiv.org/abs/2207.08204">Fast Composite Optimization and Statistical Recovery in Federated Learning.</a><br>
    Yajie Bao, <b>Michael Crawshaw</b>, Mingrui Liu
    International Conference on Machine Learning, 2022.</li>
    <li><a href="https://arxiv.org/abs/2009.09796">Multi-Task Learning with Deep Neural Networks: A Survey.</a><br>
    <b>Michael Crawshaw</b><br>
    <i>arXiv:2009.09796</i>, 2020.</li>
  </ul>

  <h2>Teaching</h2>
  <ul>
    <li> GMU Graduate Teaching Assistant:
    <ul>
        <li>CS 657: Mining Massive Datasets (Fall 2020, Fall 2021)</li>
        <li>CS 471: Operating Systems (Fall 2020, Fall 2021, Spring 2022)</li>
        <li>CS 583: Analysis of Algorithms (Spring 2021)</li>
        <li>CS 571: Operating Systems (Spring 2021)</li>
        <li>CS 330: Formal Methods and Models (Fall 2019, Spring 2020)</li>
    </ul>
    <li> OSU Undergraduate Teaching Assistant:
    <ul>
        <li>CSE 3321: Automata and Formal Languages (Summer 2017, Fall 2017, Spring 2018)</li>
    </ul>
    <li> OSU Undergraduate Honors Math Mentor:
    <ul>
        <li>Math 4181H: Honors Analysis I (Fall 2016)</li>
        <li>Math 4182H: Honors Analysis II (Spring 2017)</li>
    </ul>
  </ul>

  <h2>Misc</h2>
  <ul>
    <li>In my spare time I like to develop open-source projects to use as tools in my
    life, including a simple arXiv paper recommender and a program to generate lessons
    for language learning by converting text files into bilingual audiobooks. Both
    projects are available at my <a href="https://github.com/mtcrawshaw">GitHub
    page</a>.</li>
    <li>My personal record for the 5000m run is 16:28 (this was in 2014, but I intend to
    be this fast again one day!)</li>
    <li>Mostly, I just want to work on interesting problems and help others do the
    same.</li>
  </ul>
</body>

</html>
